# AB-test
A/B test

Data:test_basic.csv, randomization.csv, novelty_effect.csv

Notebook: ABtest1.ipynb,ABtest2.ipynb,ABtest3.ipynb

A/B Test Intro

After having extracted insights from the data, a data scientist typically develops a few hypotheses on how to improve the product. Once the hypotheses have been developed, it is then time to test them in the real world to see if they are true. This is done via A/B tests.

The idea behind A/B testing is surprisingly simple: give some percentage of users the new version of the product. Then check if these users are outperforming the users using the old version of the site. If so, go ahead with the change for all users. And it is really nothing new, the same approach ha been used for decades in pretty much all industries. What is new though is:

The cost of making a change to the product in software is extremely low

The amount of data is much larger than ever before

Those two points above lead to the fact that companies can try tons of small variations of the product and are able to estimate extremely small improvements. All major tech companies are running hundreds of A/B tests at the same time.

In this project I mainly focused on  a few specific A/B testing topics, such as novelty effect, sample size, and user randomization
